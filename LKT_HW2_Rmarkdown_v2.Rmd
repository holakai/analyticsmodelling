---
title: ""
output:
  html_document: default
  pdf_document: default
editor_options:
  chunk_output_type: console
---
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Question 4.1.
> Describe a situation or problem from your job, everyday life, current events, etc., for which a clustering model would be appropriate. List some (up to 5) predictors that you might use.

__Situation__


__Predictors__



##  Question 4.2.
> Use the R function kmeans to cluster the points as well as possible. 
(a) Report the best combination of predictors, 
(b) your suggested value of k, 
(c) and how well your best clustering predicts flower type.

__Approach__

I combine finding both (a) best combination and (b) suggest k value, through using a user defined function, to pass in different (a) combinations of columns and (b) different values of k


__Steps__

(a) Since we have 4 different predictors, the  best combination of predictors can be 2 combinations,3 combinations or the 4 (all) combination. I create a list predictorsCombis and pass in the different possible predictor combinations

(b) To find a good value of k, I loop through a range of k values. Knowing that there are only 3 types of flowers, I narrow my range of k to loop through from k=2 to 7.

* In lecture 4.4, Prof Sokol explained about the elbow diagram that is used to determine the optimal value of k (by charting the total distance of data points to its  cluster center)
* While in homework I use % correct predictions as my metric to find the most optimal value, since we are using kmeans (and typically unstructured data), I will use the elbow diagram to determine the optimal k value.

```{r q4_2_a}

rm(list=ls())
set.seed(123) 
library(tidyverse)

iris <- read.table("C:/Users/k.loh/OneDrive - SKIM/ISYE 6501/week_2_Homework-summer/week 2 data-summer/data 4.2/iris.txt", stringsAsFactors = FALSE, header=TRUE)

#Question to Menno on scaling, there seems to be two methods: 
#[1]
scdata <- iris # initialize value/size of sdata
for (i in 1:4) { scdata[,i] <- (iris[,i]-min(iris[,i]))/(max(iris[,i])-min(iris[,i])) }

#[2] Using R's scale function
scalediris<- scale(iris[,-ncol(iris)])
  
func_kmeans <- function(data, k_val) {
  pred <- c()
  iris_clusters <- kmeans(data,k_val,nstart=20) 
  
  pred <- iris_clusters$tot.withinss
  
  return(pred)
} 

#Question to Menno on piping in different column combinations into my function, I can't seem to make it such that R would read these as column combinations together (it is treating it as looping column by column... would you know how best to make sure that R is using my columns e.g. 1:2 together?)
predictorsCombis <- c(c(iris[,1:2]),c(iris[,1:3]),c(iris[,1:4]))
k_list <- c(2:5)
kmeans_accu <- c()

for (predictorsCombi in predictorsCombis){
  for (k_val in k_list) {
  kmeans_accu <- c(kmeans_accu, func_kmeans(predictorsCombi, k_val))
  }
}

kmeans_accu
```

__Observations__
From the elbow diagram, there is a kink at k=3, where after the marginal benefit of increasing the cluster size is small, hence I use k=3 as the suggested value.



> (b)your suggested value of k




```{r q2_4_2b, cache=TRUE}


```

__Output__

> (c) and how well your best clustering predicts flower type.

To claculate how well k=3 predicts flower type, I use the % of between_ss/ total_ss output from kmeans.

Background resource I found online: between_ss means high variance between the clusters (hence low similarity between groups) and total_ss is the sum of squared distances of each observation to the overall sample avg
(https://discuss.analyticsvidhya.com/t/interpretation-result-of-k-means-algorithm/17750) 
(https://stats.stackexchange.com/questions/82776/what-does-total-ss-and-between-ss-mean-in-k-means-clustering)


```{r q2_4_2b, cache=TRUE}

# Test C on a wide range of values

kmeans(iris[,1:4],3,n=20)
```

From the output the percentage of 88.4% indicates that k=3 is a good fit 

Within cluster sum of squares by cluster:
[1] 39.82097 23.87947 15.15100
 (between_SS / total_SS =  88.4 %)
 
 
 # Bonus calculations -> compare classification with actual data in column 5

##  Question 5.1.
> Using crime data from the file uscrime.txt (http://www.statsci.org/data/general/uscrime.txt, description at http://www.statsci.org/data/general/uscrime.html), test to see whether there are any outliers in the last column (number of crimes per 100,000 people).  Use the grubbs.test function in the outliers package in R.

Found some helpful resources:
https://www.youtube.com/watch?v=kc9O5XfV2pc
https://www.statology.org/grubbs-test-r/
https://statsandr.com/blog/outliers-detection-in-r/
https://statsandr.com/blog/outliers-detection-in-r/
https://stats.stackexchange.com/questions/3136/how-to-perform-a-test-using-r-to-see-if-data-follows-normal-distribution


__Background reading__
I read up on Grubb's test given that it was not specifically mentioned in our lecture.
Grubb's test assumes that the dataset is normally distributed. Hence, my approach is to first check to see if the 'Crime' column in our crimes data is normally distributed

To check if the 'Crime' column is normally distributed, I use:
* Histogram - to visualize the data's distribution
* Additional tests using: shapiro.test
* Additional test using: Q-Q plot


```{r q5_1}
#Load grubbs.test that is in the 'outliers' package
set.seed(125)
library(outliers)

crimes <- read.table("C:/Users/k.loh/OneDrive - SKIM/ISYE 6501/week_2_Homework-summer/week 2 data-summer/data 5.1/uscrime.txt", stringsAsFactors = FALSE, header=TRUE)

#Use a boxplot to detect any outliers
boxplot(crimes$Crime, ylab="Crime rate: number of offenses per 100,000 population in 1960")

#Use histogram to check data's distrbution
ggplot(crimes) + aes(x=Crime) + geom_histogram()
hist(crimes$Crime)

#Question to Menno: Sohuld the shapiro test be tested on the dataset with the outliers REMOVED? 
#Since from the crimes boxplot, I can remove the 2 values that look like outliers
#Other questions are generally on how to interpret the different results based on shapiro, qq, and grubbstest

#Alternative tests to test normality
shapiro.test(crimes$Crime)

```

__Observations__

* The box plot indicates 2 outliers around the 2,000 value
* From the histogram, the crime rate skews more to the left, and doesn't follow a normal distribution

Results from additional normality tests:

	Shapiro-Wilk normality test

data:  crimes$Crime
W = 0.91273, p-value = 0.001882

Taking an alpha-value of 0.05, since the Shapiro-Wilk null hypothesis is that the data is normally distributed, since the p-value is <0.05, the null hypothesis is rejected, meaning that the data is not normally distributed.

```{r q5_1_b}

qqnorm(crimes$Crime)

#type=10 tests for one outlier
grubbs.test(crimes$Crime, type=20)

```

__Observations__

* The Q-Q plot 

##  Question 6.1.
> Describe a situation or problem from your job, everyday life, current events, etc., for which a Change Detection model would be appropriate. Applying the CUSUM technique, how would you choose the critical value and the threshold?

Change detection - 

##  Question 6.1.
> 1.	Using July through October daily-high-temperature data for Atlanta for 1996 through 2015, use a CUSUM approach to identify when unofficial summer ends (i.e., when the weather starts cooling off) each year.  

__Approach__

Since we are looking for when summer ends (weather starts cooling off), the cusum formula I use would be the one that detects decreases. The 3 variables I need are: mean, 
Since we have year on year data, I calculate cusum for each year
To use the cusum formula, I use July average as the mean

As mean is the expectation for summer, I use average temperature in July (assuming July is always summer) as the proxy for summer

```{r q6_2}
set.seed(126)

#qcc package for cusum
library(qcc)
data <- read.table("C:/Users/k.loh/OneDrive - SKIM/ISYE 6501/week_2_Homework-summer/week 2 data-summer/data 6.2/temps.txt", stringsAsFactors = FALSE, header=TRUE)



```


> 2. Use a CUSUM approach to make a judgment of whether Atlantaâ€™s summer climate has gotten warmer in that time (and if so, when).


__Conclusion:__
